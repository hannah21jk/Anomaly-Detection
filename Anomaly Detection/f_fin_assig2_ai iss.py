# -*- coding: utf-8 -*-
"""fin_assig2_ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pebaaq9b4xLXHKxkaVwkEApdYIcmyRwN
"""

# Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Ignore Warnings
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("/content/cybersecurity_attacks.csv")

df.head(5)

# Name of Columns
df.columns

# Shape of data
print(f"There are {df.shape[0]}, row and {df.shape[1]} columns in the dataset")

# Dataset Info
df.info()

df.isnull().sum().sort_values(ascending=False)

# Determine recent activity
df['Alerts/Warnings'] = df['Alerts/Warnings'].apply(lambda x: 'yes' if x == 'Alert Triggered' else 'no')

df['Malware Indicators'] = df['Malware Indicators'].apply(lambda x: 'No Detection' if pd.isna(x) else x)

df['Proxy Information'] = df['Proxy Information'].apply(lambda x: 'No proxy' if pd.isna(x) else x)

df['Firewall Logs'] = df['Firewall Logs'].apply(lambda x: 'No Data' if pd.isna(x) else x)

df['IDS/IPS Alerts'] = df['IDS/IPS Alerts'].apply(lambda x: 'No Data' if pd.isna(x) else x)

#check sum of missing values
df.isnull().sum().sort_values(ascending=False)
#All Missing Values are removed.

#Explore the Device Information ColumnÂ¶
df['Device Information'].value_counts()

# Extract 'Device'
df['Browser'] = df['Device Information'].str.split('/').str[0]

#We've created the Browser column.
df['Browser']

import re
# OS and device patterns to search for
patterns = [
    r'Windows',
    r'Linux',
    r'Android',
    r'iPad',
    r'iPod',
    r'iPhone',
    r'Macintosh',
]

def extract_device_or_os(user_agent):
    for pattern in patterns:
        match = re.search(pattern, user_agent, re.I)  # re.I makes the search case-insensitive
        if match:
            return match.group()
    return 'Unknown'  # Return 'Unknown' if no patterns match

# Extract device or OS
df['Device/OS'] = df['Device Information'].apply(extract_device_or_os)

# Display the extracted device or OS
df['Device/OS']

#We've created the Device/OS column.
df['Browser'].value_counts()

df['Device/OS'].value_counts()

#Dropping the Device Information Column
df = df.drop('Device Information', axis = 1)

df['Device/OS'].value_counts()

def extract_time_features(df, Timestamp):
    # Convert timestamp column to datetime if it's not already
    df[Timestamp] = pd.to_datetime(df[Timestamp])

    # Extract time features
    df['Year'] = df[Timestamp].dt.year
    df['Month'] = df[Timestamp].dt.month
    df['Day'] = df[Timestamp].dt.day
    df['Hour'] = df[Timestamp].dt.hour
    df['Minute'] = df[Timestamp].dt.minute
    df['Second'] = df[Timestamp].dt.second
    df['DayOfWeek'] = df[Timestamp].dt.dayofweek

    return df

# Call the function and store the result in a new DataFrame
new_df = extract_time_features(df, 'Timestamp')

# Check if new columns are created
print(new_df.head())

df.head().T

df.describe(include = 'object').T

# Checking the Day Column ploting with plotly
plt = px.histogram(df, x = 'Day', color = 'Malware Indicators', title = 'Number of Malware Attacks by Day')
plt.show()

# month Distribution
plt = px.histogram(df, x = 'Month', title = 'Month')
plt.show()

# Checking the Month Column ploting with plotly
plt = px.histogram(df, x = 'Month', color = 'Malware Indicators', title = 'Number of Malware Attacks by Month')
plt.show()

# Year Distrition
plt = px.histogram(df, x='Year', title = 'Year')
plt.show()

# Checking the Protocol distribution with Bar Chart Using Plotly
plt = px.histogram(df, x = 'Protocol', color = 'Malware Indicators', title = 'Number of Malware Attacks by Protocol')
plt.show()

# Traffic Distribution
plt = px.pie(df, names = 'Traffic Type', title = 'Traffic Distribution')
plt.show()

# Ploting the Traffic Type distribution with Bar Chart Using Plotly
plt = px.histogram(df, x = 'Traffic Type', color = 'Malware Indicators', title = 'Number of Malware Attacks by Traffic Type')
plt.show()

# Attack Type Distribution
plt = px.pie(df, names = 'Attack Type', title = 'Attack Type Distribution')
plt.show()

# Checking the attack types distribution with Bar Chart Using Plotly
plt = px.histogram(df, x='Attack Type', color='Traffic Type', title='Number of Malware Attacks by Attack Type')
plt.show()

# Browsers Distribution
plt = px.pie(df, names = 'Browser', title = 'Browser Distribution')
plt.show()

# Platform Distribution
plt = px.pie(df, names = 'Device/OS', title = 'Platform Distribution')
plt.show()

# Checking the Browser and Devices with Attack Type distribution with Bar Chart Using Plotly
plt = px.histogram(df, x= 'Device/OS', color = 'Attack Type', title = 'Number of Malware Attacks by Browser and Devices')
plt.show()

dns_filter = df['Traffic Type'] == 'DNS'

filtered_df = df[dns_filter]

# Filter for DNS traffic
dns_filter = df['Traffic Type'].str.strip().str.upper() == 'DNS'
filtered_df = df[dns_filter]
# Now 'filtered_df' contains only rows with DNS traffic.

filtered_df = df[df['Traffic Type'] == 'DNS']
print(filtered_df.shape)

# Convert 'Timestamp' to datetime format
filtered_df['Timestamp'] = pd.to_datetime(filtered_df['Timestamp'])

# Group by time intervals ( 1-minute bins) and count queries
query_volume1 = filtered_df.set_index('Timestamp').resample('1Min').size()

print(query_volume1.head())

# Plot the query volume over time
import matplotlib.pyplot as plt

query_volume1.plot(figsize=(12, 6), title='DNS Query Volume Over Time')
plt.xlabel('Time')
plt.ylabel('Number of Queries')
plt.show()

# show the first few records
print(query_volume1.head())

query_volume = filtered_df.set_index('Timestamp').resample('15Min').size()
print(query_volume.head())

# Plot the query volume over time
import matplotlib.pyplot as plt

query_volume.plot(figsize=(12, 6), title='DNS Query Volume Over Time')
plt.xlabel('Time')
plt.ylabel('Number of Queries')
plt.show()

# show the first few records
print(query_volume.head())

import matplotlib.pyplot as plt

query_volume.plot()
plt.title('Query Volume Over Time')
plt.xlabel('Time')
plt.ylabel('Number of Queries')
plt.show()

# Ensure 'Timestamp' is in datetime format
filtered_df['TTL'] = pd.to_datetime(filtered_df['Timestamp'])

# Create a reference timestamp  in the dataset
start_timestamp = filtered_df['TTL'].min()

# Calculate the difference between each timestamp and the reference timestamp in seconds
filtered_df['TimeSinceStart'] = (filtered_df['TTL'] - start_timestamp).dt.total_seconds()

# Print out the TimeSinceStart values to inspect
print("TimeSinceStart values:")
print(filtered_df[['Timestamp', 'TimeSinceStart']])

# Now check the range of TimeSinceStart
print(f"Min TimeSinceStart: {filtered_df['TimeSinceStart'].min()}")
print(f"Max TimeSinceStart: {filtered_df['TimeSinceStart'].max()}")

# Calculate the TTL based on the time difference and your conditions
# Ensure TTL does not go negative, and if TimeSinceStart is small, TTL should not drop to zero
filtered_df['TTL'] = 300 - filtered_df['TimeSinceStart']  # 300 seconds = 5 minutes
# Ensure TTL is not negative
filtered_df['TTL'] = filtered_df['TTL'].clip(lower=0)

# Drop the temporary column
filtered_df = filtered_df.drop('TimeSinceStart', axis=1)

#Check if the timestamps have regular or irregular intervals
# Ensure Timestamp is datetime format
df['Timestamp'] = pd.to_datetime(df['Timestamp'])
# Find time difference between consecutive queries
df['TimeDiff'] = df['Timestamp'].diff()
# Check time gap distribution
print(df['TimeDiff'].describe())

#Check if the domain names in your DNS traffic are unique or repetitive
# Assuming 'Source IP' or 'Destination IP' contains domain information
domain_counts = df['Source IP Address'].value_counts()
print(df['Source IP Address'].nunique())  # Number of unique source IP addresses
# Display top 10 most frequent domains
print(domain_counts.head(10))

import pandas as pd
import random
import time

# Generate synthetic DNS traffic data
data = []
# Generate 100 rows of DNS traffic data
for _ in range(100):
    # Random timestamps
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(random.randint(1, 1000000000)))
    # Random DNS queries
    query = random.choice(['example.com', 'google.com', 'github.com'])
    # Random DNS responses
    response = random.choice(['192.168.1.1', '172.217.5.68', '140.82.121.3'])
    # Random TTL values between 100 seconds and 1 hour
    ttl = random.randint(100, 3600)
    data.append([timestamp, query, response, ttl, 'DNS'])  # Append DNS traffic data

# Create a DataFrame
df = pd.DataFrame(data, columns=['Timestamp', 'Query', 'Response', 'TTL', 'Traffic Type'])

# Now filter for DNS traffic
filtered_df = df[df['Traffic Type'] == 'DNS']

# Display the shape of the filtered DNS traffic
print(filtered_df.shape)
print(filtered_df.head())

# Summary of TTL values
print(filtered_df['TTL'].describe())

# Find the most common DNS queries
most_common_queries = filtered_df['Query'].value_counts()
print("Most common queries:\n", most_common_queries)

# Check for the distribution of TTL values
import matplotlib.pyplot as plt

filtered_df['TTL'].hist(bins=20)
plt.title('Distribution of TTL Values')
plt.xlabel('TTL')
plt.ylabel('Frequency')
plt.show()

import numpy as np
from collections import Counter

# Use 'Query' column for entropy calculation
query_values = filtered_df['Query']

# Calculate frequency distribution of query values
query_counts = Counter(query_values)

# Calculate the total number of queries
total_queries = len(query_values)

# Calculate the entropy
entropy = -sum((count / total_queries) * np.log2(count / total_queries)
               for count in query_counts.values())

print(f"Query Entropy: {entropy}")

import matplotlib.pyplot as plt

query_volume.plot(figsize=(12, 6), title='DNS Query Volume Over Time')
plt.xlabel('Time')
plt.ylabel('Number of Queries')
plt.grid(True)
plt.tight_layout()
plt.show()

# Convert 'Timestamp' column to datetime format if it's not already
filtered_df['Timestamp'] = pd.to_datetime(filtered_df['Timestamp'])

# Set 'Timestamp' as the index
filtered_df.set_index('Timestamp', inplace=True)

# Resample the data to get the number of DNS queries per hour
dns_queries_per_hour = filtered_df.resample('H').size()

# Plot DNS traffic over time
dns_queries_per_hour.plot(kind='line', title='DNS Traffic Over Time')
plt.xlabel('Time')
plt.ylabel('Number of DNS Queries')
plt.show()

# Plot TTL values over time for DNS queries
filtered_df['TTL'].plot(kind='line', title='TTL Values Over Time', figsize=(10, 6))
plt.xlabel('Timestamp')
plt.ylabel('TTL Value')
plt.show()

from sklearn.ensemble import IsolationForest

# Select TTL as the feature for anomaly detection
X = filtered_df[['TTL']]

# Train an Isolation Forest model
model = IsolationForest(contamination=0.05)  # Assume 5% of data are anomalies
filtered_df['Anomaly'] = model.fit_predict(X)

# Mark anomalies as -1 and normal points as 1
anomalies = filtered_df[filtered_df['Anomaly'] == -1]
print(f"Detected anomalies: {anomalies}")

# Plot anomalies
# Reset the index to make 'Timestamp' a column again
filtered_df = filtered_df.reset_index()
plt.scatter(filtered_df['Timestamp'], filtered_df['TTL'], c=filtered_df['Anomaly'], cmap='coolwarm')
plt.title('Anomaly Detection in DNS TTL')
plt.xlabel('Timestamp')
plt.ylabel('TTL Value')
plt.show()

from sklearn.preprocessing import StandardScaler, LabelEncoder
filtered_df = extract_time_features(filtered_df, 'Timestamp')
# Drop the original Timestamp column
filtered_df = filtered_df.drop(columns=['Timestamp'])

# Encode Query, Response, and Traffic Type using LabelEncoder
label_enc_query = LabelEncoder()
filtered_df['Query_encoded'] = label_enc_query.fit_transform(filtered_df['Query'])

label_enc_response = LabelEncoder()
filtered_df['Response_encoded'] = label_enc_response.fit_transform(filtered_df['Response'])

label_enc_traffic = LabelEncoder()
filtered_df['Traffic_Type_encoded'] = label_enc_traffic.fit_transform(filtered_df['Traffic Type'])

# Drop original categorical columns
filtered_df = filtered_df.drop(columns=['Query', 'Response', 'Traffic Type'])

scaler = StandardScaler()
filtered_df['TTL_normalized'] = scaler.fit_transform(filtered_df[['TTL']])

# Drop the original TTL column
filtered_df = filtered_df.drop(columns=['TTL'])

# iloc is to slice the DataFrame in Pandas , iloc[rows,cols]
# x has all features
# y has only the last column [status] (labeled data)
X=filtered_df.iloc[:,:-1]
y=filtered_df.iloc[:,-1:]

from sklearn.model_selection import train_test_split

# Split the data (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)

print(f"Training Features Shape: {X_train.shape}")
print(f"Testing Features Shape: {X_test.shape}")
print(f"Training Labels Shape: {y_train.shape}")
print(f"Testing Labels Shape: {y_test.shape}")

print(y_binned.value_counts())

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# Assuming 'Anomaly' is the target variable
target_column = 'Anomaly'

# Feature columns (excluding target and any previously removed columns)
feature_columns = [col for col in filtered_df.columns if col not in [target_column, 'Query', 'Response', 'Traffic Type']]  # Exclude removed columns

# Encode the target variable
label_enc_target = LabelEncoder()
filtered_df[target_column] = label_enc_target.fit_transform(filtered_df[target_column])

# Selecting feature columns and target
X = filtered_df[feature_columns]
y = filtered_df[target_column]

# Split the data into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# the model RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# Feature importance
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

# Print the feature importance
print("\nFeature Importance:")
for f in range(X_train.shape[1]):
    print(f"{filtered_df.columns[indices[f]]}: {importances[indices[f]]:.4f}")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Build the ANN model
model = Sequential()

# Add input layer (input_dim = number of features in the dataset)
model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))

# Add hidden layers
model.add(Dense(units=32, activation='relu'))
model.add(Dense(units=16, activation='relu'))

# Add output layer (for binary classification, use 'sigmoid' activation)
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, validation_data=(X_test_scaled, y_test))

# Make predictions
y_pred = model.predict(X_test_scaled)
# Convert probabilities to binary (0 or 1)
y_pred = (y_pred > 0.5)
# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# Plotting the loss and accuracy during training
plt.figure(figsize=(12, 6))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Test Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import classification_report
import pandas as pd

# Train an SVM classifier
svm_model = SVC(kernel='linear', random_state=30)  # You can use 'linear' or other kernels like 'rbf', 'poly'

# Fit the model on training data
svm_model.fit(X_train_scaled, y_train)

# Make predictions
y_pred_svm = svm_model.predict(X_test_scaled)

# Evaluate the model
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f"SVM Accuracy: {accuracy_svm:.2f}")

# Get classification report as a DataFrame
report = classification_report(y_test, y_pred_svm, output_dict=True)
report_df = pd.DataFrame(report).transpose()

# Plot precision, recall, F1-score
report_df[['precision', 'recall', 'f1-score']].plot(kind='bar', figsize=(10, 6))
plt.title("SVM Classification Metrics")
plt.xlabel("Class")
plt.ylabel("Score")
plt.xticks(rotation=0)
plt.show()